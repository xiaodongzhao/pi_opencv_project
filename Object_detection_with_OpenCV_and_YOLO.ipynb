{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This project will show how to use YOLO (You Only Look Once) object detector to detect objects in both images and streams from a camera, using Python, OpenCV and deep learning.\n",
    "\n",
    "We can run this either on a Windows laptop with intergrated camera, or Raspberry Pi with USB camera.\n",
    "\n",
    "This code is hosted at: https://github.com/xiaodongzhao/pi_opencv_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "- [Python 3.x](https://www.python.org/downloads/): the programing language\n",
    "- [Numpy](https://www.numpy.org/): for matrix operations\n",
    "- [Matplotlib](https://matplotlib.org/): for plotting\n",
    "- [OpenCV](https://opencv.org/): image processing\n",
    "- [Configuration and pre-trained weights for YOLO v3 and YOLO v3 tiny](https://pjreddie.com/darknet/yolo/): for object detection\n",
    "- [Raspberry Pi](https://www.raspberrypi.org/) with USB camera (optional)\n",
    "- or Windows laptop with camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Enviroment setup\n",
    "2. Build YOLO object detector using OpenCV\n",
    "3. Use YOLO to detect objects from an image\n",
    "4. Use YOLO to detect objects from a camera "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enviroment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Python and OpenCV\n",
    "\n",
    "- Python: an interpreted, high-level, general-purpose programming language, from [Python wikipedia](https://en.wikipedia.org/wiki/Python_(programming_language)).\n",
    "\n",
    "- OpenCV: (Open source computer vision) is a library of programming functions mainly aimed at real-time computer vision. OpenCV supports the deep learning frameworks TensorFlow, Torch/PyTorch and Caffe, from [OpenCV wikipedia](https://en.wikipedia.org/wiki/OpenCV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "https://www.anaconda.com/distribution/\n",
    "![](figures/anaconda_download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "1 | 2 \n",
    "- | - \n",
    "![alt](figures/anaconda_install_1.png) | ![alt](figures/anaconda_install_2.png)\n",
    "![alt](figures/anaconda_install_3.png) | ![alt](figures/anaconda_start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "pip3 install --user numpy matplotlib jupyter notebook \n",
    "pip3 install --user opencv-contrib-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After everything is installed, \n",
    "navigate to the folder containing this file.\n",
    "\n",
    "Type in:\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "A browser should pop up and within the browser you can open this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## YOLO\n",
    "\n",
    "- [You Only Look Once (YOLO)]((https://pjreddie.com/darknet/yolo/)) is a state-of-the-art, real-time object detection system.\n",
    "\n",
    "- We will use YOLO v3 pre-trained on the COCO dataset, which consists of 80 labels, including: *person, bicycle, car, motorbike, aeroplane, bus, train, truck*... and so on.\n",
    "- weights can be downloaded from [YOLO](https://pjreddie.com/media/files/yolov3.weights) and [YOLO-tiny](https://pjreddie.com/media/files/yolov3-tiny.weights) and saved into the *yolo_coco* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Raspberry Pi: \n",
    "\n",
    "[Raspberry Pi](https://www.raspberrypi.org/) is a low cost, credit-card sized computer, which we can run a Linux system on. \n",
    "\n",
    "[Raspberry Pi 3 Model B](https://www.raspberrypi.org/products/raspberry-pi-3-model-b/) has the following [specs](https://www.raspberrypi.org/documentation/hardware/raspberrypi/):\n",
    "\n",
    "- Quad Core 1.2GHz Broadcom BCM2837 64bit CPU\n",
    "- 1GB RAM\n",
    "- BCM43438 wireless LAN and Bluetooth Low Energy (BLE) on board\n",
    "- 100 Base Ethernet\n",
    "- 40-pin extended GPIO\n",
    "- 4 USB 2 ports\n",
    "- 4 Pole stereo output and composite video port\n",
    "- Full size HDMI\n",
    "- CSI camera port for connecting a Raspberry Pi camera\n",
    "- DSI display port for connecting a Raspberry Pi touchscreen display\n",
    "- Micro SD port for loading your operating system and storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figures/RPI_model_B.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build YOLO object detector using OpenCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "I didn't create this from scratch, most of codes are based on: \n",
    "\n",
    "- [YOLO object detection with OpenCV](https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/) from **pyimagesearch** by *Adrian Rosebrock*\n",
    "- [Basic motion detection and tracking with Python and OpenCV](https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/) from **pyimagesearch** by *Adrian Rosebrock*\n",
    "- [Convolutional Neural Networks\n",
    "](https://www.coursera.org/learn/convolutional-neural-networks) by [deeplearning.ai](https://www.deeplearning.ai/)\n",
    "- [YOLO](https://pjreddie.com/darknet/yolo) by *Joseph Redmon, Ali Farhadi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load YOLO model\n",
    "\n",
    "All the files we need including sample images, pre-trained weights and configurations are included in the current folder. \n",
    "\n",
    "To see what is in the current folder, run *tree*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "if os.name==\"nt\": #windows\n",
    "    print(subprocess.check_output(\"tree /a /f\", shell=True).decode())\n",
    "else:\n",
    "    print(subprocess.check_output(\"tree\", shell=True).decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Files we are going to use are:\n",
    "\n",
    "- **yolo_coco/** : The YOLOv3 object detector pre-trained (on the COCO dataset) model files.\n",
    "- **images/**: some example images we can perform object detection on. \n",
    "- **scripts/**: script to download YOLO weights. \n",
    "- **Object_detection_with_OpenCV_and_YOLO.ipynb**: this file we will be working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We would need to load the class labels from COCO dataset first, then generate some random colors for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load the COCO class labels the YOLO model was trained on\n",
    "with open(\"yolo_coco/coco.names\") as f:\n",
    "    coco_labels = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# initialize a list of colors to represent each possible class label\n",
    "# seed random number so we have consistent results\n",
    "np.random.seed(0)\n",
    "colors = np.random.randint(0, 255, size=(len(coco_labels), 3), dtype=\"uint8\")\n",
    "\n",
    "print(\"%d labels are: %s...\" % (len(coco_labels), \", \".join(coco_labels[0:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To create the YOLO model in OpenCV, we can use OpenCV's DNN function called [cv2.dnn.readNetFromDarknet](https://docs.opencv.org/4.0.0/d6/d0f/group__dnn.html#gafde362956af949cce087f3f25c6aff0d), which requires: \n",
    "- path to the .cfg file with text description of the network architecture \n",
    "- path to the .weights file with learned network.\n",
    "\n",
    "Both files are stored under the **yolo_coco** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load YOLO object detector trained on COCO dataset (80 classes)\n",
    "if os.name==\"posix\" and os.uname()[4].startswith(\"arm\"):\n",
    "    # due to memoery limit on RPI, load the yolo tiny model, which run faster with less accuracy\n",
    "    yolo_v3 = cv2.dnn.readNetFromDarknet(\"yolo_coco/yolov3-tiny.cfg\", \"yolo_coco/yolov3-tiny.weights\")\n",
    "    print(\"Using YOLO v3 tiny\")\n",
    "else:\n",
    "    yolo_v3 = cv2.dnn.readNetFromDarknet(\"yolo_coco/yolov3.cfg\", \"yolo_coco/yolov3.weights\")\n",
    "    print(\"Using YOLO v3\")\n",
    "\n",
    "# 'YOLO' layers are not connected to following layers, we need this to get the detection result\n",
    "all_layers = yolo_v3.getLayerNames()\n",
    "yolo_layers = [all_layers[i - 1] for i in yolo_v3.getUnconnectedOutLayers().flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are 107 layers in YOLO v3 and 23 layers in YOLO v3 tiny, we can see their type and order by:\n",
    "\n",
    "> yolo_v3.getLayerNames()\n",
    "\n",
    "There are special 'YOLO' layers in the network, each 'YOLO' layer is in charge of detect objects in different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use YOLO to detect objects from an image.\n",
    "\n",
    "Let's load an example image under the *images/* folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load an image from either a camera or a file\n",
    "def load_image(src):\n",
    "    if isinstance(src, int): \n",
    "        # create an OpenCV VideoCapture object, which can grab image from camera\n",
    "        cap_id = \"cap%d\"%src\n",
    "        if not hasattr(load_image, cap_id):  \n",
    "            setattr(load_image, cap_id, cv2.VideoCapture(src))\n",
    "        _, image = getattr(load_image, cap_id).read()           \n",
    "    else: # open an file\n",
    "        image = cv2.imread(src)\n",
    "    \n",
    "    # resize image\n",
    "    if image is not None:\n",
    "        image = cv2.resize(image,(416,416))\n",
    "    return image\n",
    "\n",
    "# plot an OpenCV image using Matplotlib\n",
    "def plot_cv_image(image):\n",
    "    # because OpenCV use BGR order, we need to convert it to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    plt.imshow(image_rgb)\n",
    "    ax.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "image = load_image(\"images/giraffe.jpg\")\n",
    "plot_cv_image(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An image from file or camera typically has a dimension of (height, width, channel), for example (576, 768, 3), where 3 represents RGB channels. \n",
    "\n",
    "However, OpenCV YOLO model would need input of dimension of (number of images, channel, height, width). We can use [blobFromImage](https://docs.opencv.org/3.3.0/d6/d0f/group__dnn.html#ga0507466a789702eda8ffcdfa37f4d194) function to do the conversion for one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# detect a single image using the yolo_v3 model\n",
    "def detect_image(yolo_model, single_image, output_layers):\n",
    "    # construct a blob from the input image\n",
    "    blob = cv2.dnn.blobFromImage(single_image, scalefactor=1 / 255.0, size=(416, 416), swapRB=True, crop=False)\n",
    "    # perform a forward of the YOLO object detector\n",
    "    yolo_model.setInput(blob)\n",
    "    # output bounding boxes and associated probabilities\n",
    "    yolo_outputs = yolo_model.forward(outBlobNames=output_layers)\n",
    "    # outputs is a list of lenght 3, correspoding results from 3 YOLO layers, merge them together\n",
    "    yolo_outputs = np.vstack(yolo_outputs)\n",
    "    return yolo_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "yolo_outputs = detect_image(yolo_v3, image, yolo_layers)\n",
    "end_time = time.time()\n",
    "print(\"YOLO took %.2f seconds.\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The outputs from YOLO model have a dimension of (10647, 85), because YOLO detects an image (416, 416) with three grid sizes: 13x13, 26x26 and 52x52 grids, where each grid box would have 3 results (3 anchor boxes). So for each image, we would have (13x13+26x26+52x52)x3 = 10647 detections.\n",
    "\n",
    "For each box detection, YOLO would output 85 attributes: \n",
    "\n",
    "- The first 4 values of each object is the bounding box: center (x, y), followed by width and height, relative to the size of the image.\n",
    "- The 5th value is the bounding box confidence.\n",
    "- The following 80 values are class confidences, remember we have 80 classes on the COCO dataset.\n",
    "\n",
    "As we can see, there are two many object detected, of which some are low confidence boxes, and some are overlapping boxes. We will need to filter and remove them.\n",
    "\n",
    "Low confidence boxes can be removed by setting a threshold on bounding box confidence, while overlapping boxes can be removed by non maximum suppression using an OpenCV function [NMSBoxes](https://docs.opencv.org/4.1.0/d6/d0f/group__dnn.html#ga9d118d70a1659af729d01b10233213ee)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# filter yolo detection result, return a 2d array, with 7 columns: x,y,w,h,conf,class, class_conf\n",
    "def filter_yolo_outputs(yolo_outputs, image_shape):\n",
    "    # when there is no object detected, simply return empty array\n",
    "    if len(yolo_outputs) == 0:\n",
    "        return np.empty((0, 7))\n",
    "    # only keep the boxes with confidence>0.5\n",
    "    filtered = yolo_outputs[yolo_outputs[:, 4] > 0.5]\n",
    "    # scale the bounding box coordinates back to pixels\n",
    "    image_h, image_w, _ = image_shape\n",
    "    filtered[:, 0:4] = filtered[:, 0:4] * np.array([image_w, image_h, image_w, image_h])\n",
    "    # convert the center to the left-top corner\n",
    "    filtered[:, 0] = filtered[:, 0] - filtered[:, 2] / 2\n",
    "    filtered[:, 1] = filtered[:, 1] - filtered[:, 3] / 2\n",
    "    # select the class with highest class confidence\n",
    "    class_confidences = filtered[:, 5:].max(axis=1)\n",
    "    class_ids = filtered[:, 5:].argmax(axis=1).astype(int)\n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    indices = cv2.dnn.NMSBoxes(filtered[:, 0:4].astype(int).tolist(), class_confidences.tolist(), score_threshold=0.5, nms_threshold=0.4)\n",
    "    # if after non-maxima suppression, no detection was found, return empty result\n",
    "    if len(indices) == 0:\n",
    "        return np.empty((0, 7))\n",
    "    indices = indices.flatten()\n",
    "    # create new array to save the filtered detection results\n",
    "    detections = np.zeros([indices.size, 7])\n",
    "    detections[:, :5] = filtered[indices, :5]  # x,y,w,h,conf\n",
    "    detections[:, 5] = class_ids[indices]  # class id\n",
    "    detections[:, 6] = class_confidences[indices]  # class conf\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "detections = filter_yolo_outputs(yolo_outputs, image.shape)\n",
    "print(\"After filter, there are %d objects.\" % len(detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# mark the image with detection results, by putting a box around the object, and a text of the object class\n",
    "def mark_image(image, detections):\n",
    "    # ensure at least one detection exists\n",
    "    if detections.shape[0] == 0:\n",
    "        return image\n",
    "    # loop over the indexes we are keeping\n",
    "    for obj in detections:\n",
    "        # extract the bounding box coordinates\n",
    "        x, y, w, h, _, class_id, _ = obj.astype(int)\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = colors[class_id].tolist()\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        class_name = coco_labels[class_id]\n",
    "        cv2.putText(image, class_name, (x, y+h//2), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "detection_image = mark_image(image, detections)\n",
    "plot_cv_image(detection_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Above are the codes needed to perform object detection using YOLO and OpenCV. Basically, 5 steps:\n",
    "\n",
    "1. Load the YOLO model from configuration and weights.\n",
    "2. Load an image.\n",
    "3. Run the YOLO model on image(s).\n",
    "4. Filter the detection result by removing low confidence boxes and non maximum suppression.\n",
    "5. Plot the a marked image with boxes and class name.\n",
    "\n",
    "Let's try with a different image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 1. Load the YOLO model from configuration and weights\n",
    "# yolo_v3 = cv2.dnn.readNetFromDarknet(\"yolo_coco/yolov3.cfg\", \"yolo_coco/yolov3.weights\")\n",
    "# all_layers = yolo_v3.getLayerNames()\n",
    "# yolo_layers = [all_layers[i - 1] for i in yolo_v3.getUnconnectedOutLayers().flatten()]\n",
    "# 2. Load an image\n",
    "image = load_image(\"images/eagle.jpg\")\n",
    "# 3. Run the YOLO model on image(s)\n",
    "yolo_outputs = detect_image(yolo_v3, image, yolo_layers)\n",
    "# 4. Filter the detection result by removing low confidence boxes and non maximum suppression.\n",
    "detections = filter_yolo_outputs(yolo_outputs, image.shape)\n",
    "print(\"After filter, there are %d objects.\" % len(detections))\n",
    "#5. Plot the a marked image with boxes and class name.\n",
    "detection_image = mark_image(image, detections)\n",
    "plot_cv_image(detection_image)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"YOLO took %.2f seconds.\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use YOLO to detect objects from a camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using OpenCV's VideoCapture functions, we can grab an image from a camera, detect it using YOLO, filter the detection results, then show it, same steps as for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 2. Load an image\n",
    "image = load_image(0)\n",
    "# 3. Run the YOLO model on image(s)\n",
    "yolo_outputs = detect_image(yolo_v3, image, yolo_layers)\n",
    "# 4. Filter the detection result by removing low confidence boxes and non maximum suppression.\n",
    "detections = filter_yolo_outputs(yolo_outputs, image.shape)\n",
    "print(\"After filter, there are %d objects.\" % len(detections))\n",
    "#5. Plot the a marked image with boxes and class name.\n",
    "detection_image = mark_image(image, detections)\n",
    "plot_cv_image(detection_image)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"YOLO took %.2f seconds.\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix \n",
    "## A: continuously detect from a camera stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also wrap the above code in a while loop, so that we can detect image from cameras continuously. However, because Raspberry Pi has a low spec CPU, this would be very slow. I only got 0.3 FPS on a Raspberry Pi Model B. \n",
    "\n",
    "I also put motion detection in the code, so that we only run object detection when there is a change in the image content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create folder to save images\n",
    "records_dir = os.path.join(os.getcwd(), \"records\")\n",
    "if not os.path.exists(records_dir):\n",
    "    os.mkdir(records_dir)\n",
    "    \n",
    "# detect images from camera for a certain time\n",
    "def detect_camera(camera_id, run_time=20):\n",
    "    first_frame = None\n",
    "    detected_images = {}\n",
    "    # create a matplotlib figure and turn on matplotlib interative plot mode\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.ion()\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # run in a loop for certain time\n",
    "    total_time = 0\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        #2. Load an image\n",
    "        frame = load_image(camera_id)\n",
    "        if frame is None:\n",
    "            print(\"Camera read failed\")\n",
    "            break\n",
    "        # covert to gray image, then blur it\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "        # save the first image as reference, to check uf the any following image change\n",
    "        if first_frame is None:\n",
    "            first_frame = gray.copy()\n",
    "            continue\n",
    "        # compute the absolute difference between the current frame and first frame\n",
    "        frame_delta = cv2.absdiff(first_frame, gray)\n",
    "        thresh = cv2.threshold(frame_delta, 50, 255, cv2.THRESH_BINARY)[1]\n",
    "        contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 2:  #Opencv 4\n",
    "            contours = contours[0]\n",
    "        else:  # OpenCV3\n",
    "            contours = contours[1]\n",
    "        # if the contour is too small, ignore it\n",
    "        contours = [c for c in contours if cv2.contourArea(c) > 2500]\n",
    "        # so we find areas with larger size, detect the image using YOLO\n",
    "        if len(contours):\n",
    "            #3. Run the YOLO model on image(s)\n",
    "            yolo_outputs = detect_image(yolo_v3, frame, yolo_layers)\n",
    "            #4. Filter the detection result by removing low confidence boxes and non maximum suppression.\n",
    "            detections = filter_yolo_outputs(yolo_outputs, frame.shape)\n",
    "            #5. Plot the a marked image with boxes and class name.\n",
    "            # here is a little different, because we want to update the same plot in a loop\n",
    "            detection_image = mark_image(frame.copy(), detections)\n",
    "            # convert OpenCV BGR to RGB for matplotlib\n",
    "            image_rgb = cv2.cvtColor(detection_image, cv2.COLOR_BGR2RGB)\n",
    "            # calculate time duration\n",
    "            fps = 1 / (time.time() - start_time)\n",
    "            # save the detected image\n",
    "            file_name = time.strftime(\"%d-%b-%Y-%H-%M-%S\", time.localtime())\n",
    "            detected_images[file_name] = detection_image\n",
    "        else:\n",
    "            fps = 0\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # save images to disk, every 10 images\n",
    "        if len(detected_images) > 10:\n",
    "            \n",
    "            print(\"saving images to \", records_dir)\n",
    "            for file_name in detected_images:\n",
    "                cv2.imwrite(os.path.join(records_dir, \"%s.png\" % file_name), detected_images[file_name])\n",
    "            detected_images = {}\n",
    "\n",
    "        # update the current plot with new image\n",
    "        ax.clear()\n",
    "        fig.suptitle('fps=%.2f, time remaining=%.2fs' % (fps, run_time - total_time), fontsize=20)\n",
    "        fig.tight_layout()\n",
    "        ax.imshow(image_rgb)\n",
    "        ax.axis(\"off\")\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        # check how much used since begining.\n",
    "        total_time += time.time() - start_time\n",
    "        if total_time > run_time:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Open camera 0 using OpenCV, if failed, check camera device id by run !ls /dev/|grep video\n",
    "detect_camera(0, run_time=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## B: Send pictures by Email\n",
    "\n",
    "After some objects/motions are detected, we can send the captured images to our email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from os.path import basename\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import formatdate\n",
    "\n",
    "# send email with attachments using SMTP\n",
    "# based on: https://stackoverflow.com/questions/3362600/how-to-send-email-attachments\n",
    "def send_mail(smtp_server, user_name, password, send_to, subject, text, files):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = user_name\n",
    "    msg['To'] = send_to\n",
    "    msg['Date'] = formatdate(localtime=True)\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    msg.attach(MIMEText(text))\n",
    "\n",
    "    for f in files or []:\n",
    "        with open(f, \"rb\") as fp:\n",
    "            part = MIMEApplication(fp.read(), Name=basename(f))\n",
    "        # After the file is closed\n",
    "        part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
    "        msg.attach(part)\n",
    "\n",
    "    smtp = smtplib.SMTP(smtp_server)\n",
    "    smtp.ehlo()\n",
    "    smtp.starttls()\n",
    "    smtp.login(user_name, password)\n",
    "    smtp.sendmail(user_name, send_to, msg.as_string())\n",
    "    smtp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# find all the capatured files\n",
    "files = [os.path.join(records_dir,f) for f in os.listdir(records_dir) if f.endswith(\".png\")]\n",
    "\n",
    "# send using Gmail, if password not accepted, try create an application password at\n",
    "# https://myaccount.google.com/apppasswords\n",
    "send_mail(smtp_server='smtp.gmail.com:587',\n",
    "          user_name=\"your_gmail\",\n",
    "          password=\"gmail_password_or_gmail_app_password\",\n",
    "          send_to=\"recipient_email\",\n",
    "          subject=\"images from Raspberry Pi camera\",\n",
    "          text=\"capatured images\",\n",
    "          files=)\n",
    "\n",
    "# delete local images\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
